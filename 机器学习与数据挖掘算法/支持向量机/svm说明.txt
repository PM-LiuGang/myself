svm
优点 泛化错误率第 计算开销不大，结果易解释
缺点 对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处于二类问题
使用数据类型 数值型和标称型数据(A,C,B,D)
===================================
svm可以看成感知器的扩展，感知器是最小化分类误差，svm是最大化分类间隔
间隔指的是两个分离的超平面间的距离，而靠近超平面的训练样本称为支持向量
Wo + W^T * Xpos = 1

Wo + W^T * Xneg = -1 
最大化间隔 2/||w||
y^(i)(Wo + W^T * x^(i)) >= 1
===================================
使用核svm解决非线性问题
线性逻辑斯谛回归或者线性svm模型，并将线性超平面当做决策边界，无法将
样本划分为正类别或负类别
核方法基本理念：通过映射函数ys将样本的原始特征映射到一个使用样本线性可分的更高维的特征空间，在新的特征控件上训练一个线性svm模型，然后同样的映射函数ys应用于新的，未知的数据上(即使用此映射将未知数据映射到新的特征空间)，进而使用新特征空间上的线性svm模型对其进行分类
ys(x1,x2) = (z1,z2,z3) = (x1,x2,x1^2+x2^2) 映射关系

引入问题：构建新的特征空间带来非常大的计算成本，特别是在高维数据的时候，这时就用到我们称做核技巧的方法

实践中，将点积x^(i).T · x^(j) 映射为 ys(x^(i)).T · ys(x^(j)),
为了降低两点之间內积精确计算阶段的成本耗费，定义一个所谓的核函数
k(x^(i),x^(j)) = ys(x^(i)).T · ys(x^(j))

最常使用的核函数 RBF 径向基函数核 或 高斯核
K(x^(i),x^(j)) = exp(-γ * (‖x^(i)-x^(j)‖)^2) # 公式简写
γ = 1/2σ^2 待优化的自由参数

核 可以解释为一对样本之间的’相似函数‘
此处的符号将距离转换为相似性评分，而由于指数项的存在，使得相似性会
介于0(差异巨大的样本)之间和1(完全相同的样本)

泛化误差？
=============================================
