'''
Create time:
Update time:09-21
'''
优点：精度高，对异常值不敏感，无数据输入假定；
缺点：计算复杂度高，空间复杂度高
适用数值范围：数值型和标签型
##########
K近邻算法的一般流程
收集数据：可以使用任何方法
准备数据：距离计算所需要的数值，最好是结构化的数据格式
分析数据：可以使用任何方法
训练算法：此步骤不适用与K近邻算法
测试算法：计算错误率
使用算法：首先需要输入样本数据和结构化的输出结果，然后运行K近邻算法判定输入数据
分别属于哪个分类，最后应用对计算出的分类执行后续的处理
###########
对未知类别属性的数据集中的每个点依次执行以下操作
计算已知类别数据集中的点与当前点之间的距离
按照距离递增次序排序
选取与当前点距离最小的K个点
确定前K个点所在类别的出现频率
返回前K个点出现频率最高的类别作为当前点的预测分类
#############
参数化模型
通过训练数据估计参数，通过学习得到一个模式，以便在无需原始训练数据信息的情况下
对新的数据点进行分类操作
典型的参数化模型 感知器，逻辑斯谛回归 线性支持向量机
非参数化模型
无法通过一组固定的参数进行表征，而参数的数量也会随着训练数据的增加而递增
典型的非参数化模型 决策树 核SVM

KNN惰性学习算法的典型例子，属于非参数化模型的一个子类，基于实例的学习，此类模型的
特点是会训练数据进行记忆，而惰性学习则是基于实例学习的一个特例，它在学习阶段
的计算成本为0
这种基于记忆的学习算法优点在于：分类器可以快速的适应新的训练数据，不过其缺点也是
显而易见的，计算复杂度随着样本数据量的增多而呈显线性增长，除非数据集中的样本
维度有限，而且使用了高效的数据结构（如KD树），我不能忽视训练样本，因为此模型没有
训练的步骤
#############
KNN 归纳为以下几步
1 选择近邻的数量k和距离度量方法
2 找到待分类样本的k个最近邻居
3 根据最近邻的类标进行多数投票
